---
id: opencode-vllm
title: Opencode + vLLM 연계
sidebar_label: 04. Opencode + vLLM
---

# Opencode + vLLM 연계

Cursor가 IDE 중심의 대화형 도구라면, **Opencode는 CLI 기반 배치 자동화 도구**다.
vLLM은 로컬 GPU로 모델을 무료 서빙하여 반복 작업의 비용을 0으로 만든다.

---

## 1. Cursor vs Opencode 역할 분리

| | **Cursor** | **Opencode** |
|---|---|---|
| 실행 환경 | IDE (GUI) | 터미널 (CLI) |
| 주요 강점 | 파일 편집, 실시간 수정, 컨텍스트 유지 | 배치 처리, 자동화 파이프라인, Subagent |
| 적합한 작업 | 대화형 설계, 코드 수정 | 반복 Skills 실행, 테스트 자동화 |
| 모델 | Cursor 지원 모델 | OpenAI 호환 API 모두 지원 |

**두 도구를 동시에 사용하면 커버리지가 극대화된다.**
Cursor에서 설계하고 → Opencode로 배치 구현하는 흐름이 일반적이다.

---

## 2. Opencode 설치 및 기본 설정

```bash
# 설치
npm install -g opencode-ai

# 버전 확인
opencode --version
```

### 설정 파일 (`~/.opencode/config.json`)

```json
{
  "model": "anthropic/claude-opus-4-6",
  "providers": {
    "openrouter": {
      "apiKey": "sk-or-v1-...",
      "baseURL": "https://openrouter.ai/api/v1"
    },
    "local-vllm": {
      "apiKey": "not-needed",
      "baseURL": "http://localhost:8000/v1"
    }
  }
}
```

---

## 3. Skills 파일 구조 (Opencode용)

Opencode에서는 Skills를 **파일로 관리**하고 실행 시 수동으로 로드한다.

```bash
skills/
├── api_dev.md        # API 개발 Skills
├── test_gen.md       # 테스트 생성 Skills
├── code_review.md    # 코드 리뷰 Skills
└── deploy.md         # 배포 Skills
```

### Skills 적용하여 실행

```bash
# 단일 Task 실행
opencode run \
  --sysprompt skills/api_dev.md \
  --task "users 테이블 CRUD API 생성"

# JSON 출력 패턴 (파이프라인용)
opencode run \
  --model openrouter/google/gemini-flash-1.5 \
  --sysprompt skills/test_gen.md \
  --prompt "다음 코드에 대한 pytest 테스트 생성: $(cat src/users/service.py)" \
  --output tests/test_users.py
```

---

## 4. vLLM 로컬 서버 구성

GPU가 있다면 vLLM으로 로컬 모델을 서빙하여 반복 작업 비용을 0으로 만든다.

### 설치

```bash
pip install vllm
```

### 서버 실행 (OpenAI 호환 API)

```bash
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-Coder-7B-Instruct \
  --port 8000 \
  --tensor-parallel-size 1
```

### 추천 로컬 모델

| 모델 | VRAM 요구 | 특징 |
|------|-----------|------|
| Qwen2.5-Coder-7B | ~16GB | 코딩 특화, 균형 잡힌 성능 |
| Qwen2.5-Coder-3B | ~8GB | 경량, 빠른 응답 |
| DeepSeek-Coder-6.7B | ~14GB | 코드 생성 강점 |

:::tip GPU 없는 경우
GPU가 없다면 vLLM 대신 **Ollama**를 사용할 수 있다.
CPU 전용으로도 동작하지만 속도가 느리므로 극도로 반복적인 단순 작업에만 적합하다.
:::

---

## 5. Cursor에서 vLLM 연결

Cursor에서도 로컬 vLLM 서버를 직접 사용할 수 있다.

```
Cursor Settings → Models → Add Custom Model

URL:     http://localhost:8000/v1
API Key: token-abc       (아무 값이나 입력)
Model:   Qwen2.5-Coder-7B-Instruct
```

---

## 6. 통합 Serving API (선택 사항)

클라우드 모델과 로컬 모델을 **단일 엔드포인트로 통합**하면 도구별 설정 없이 모델 전환이 가능하다.

```python
# FastAPI로 통합 Serving API 래퍼 구성
from fastapi import FastAPI
import httpx

app = FastAPI()

MODELS = {
    "local":      "http://localhost:8000/v1",       # vLLM
    "openrouter": "https://openrouter.ai/api/v1"    # 클라우드
}

@app.post("/v1/chat/completions")
async def proxy(request: dict, model_tier: str = "local"):
    base_url = MODELS[model_tier]
    async with httpx.AsyncClient() as client:
        return await client.post(
            f"{base_url}/chat/completions",
            json=request,
            timeout=120
        )
```

```bash
# 실행
uvicorn main:app --port 9000

# 사용 (Cursor/Opencode 공통)
URL: http://localhost:9000/v1?model_tier=local    # 로컬
URL: http://localhost:9000/v1?model_tier=openrouter  # 클라우드
```

---

## 7. Lazy JSON 패턴 — 파이프라인 자동화

Opencode에서 구조화된 출력을 받아 다음 단계로 연결하는 패턴이다.

```bash
# 1단계: Task 목록 생성 (Opus)
opencode run \
  --model openrouter/anthropic/claude-opus-4-6 \
  --prompt "다음 요구사항을 독립적인 개발 Task 목록으로 분리하고 JSON으로 반환: $(cat requirements.md)" \
  --output tasks.json

# 2단계: 각 Task 병렬 실행 (경량 모델)
cat tasks.json | jq -c '.[]' | while read task; do
  opencode run \
    --model openrouter/google/gemini-flash-1.5 \
    --sysprompt skills/api_dev.md \
    --prompt "다음 Task를 구현: $task" &
done
wait
```
